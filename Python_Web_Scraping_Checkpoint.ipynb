{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/pDX5l9vsTOSOQa1gPFua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ikwuegbu/Git-Checkpoint/blob/main/Python_Web_Scraping_Checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2xS8hPyK3S7",
        "outputId": "d56270aa-7ebf-44b2-a7cb-9db44970b008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to get and parse HTML content from a wikipedia page\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_html_content(wiki_url):\n",
        "    response = requests.get(wiki_url)\n",
        "    if response.status_code == 200:\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "    else:\n",
        "        raise Exception(f\"Failed to retrieve content. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "6AFRDi9xRhdZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to extract article title\n",
        "def extract_article_title(soup):\n",
        "    # Title of the Wikipedia article is in the <h1> tag\n",
        "    title = soup.find('h1').text\n",
        "    return title"
      ],
      "metadata": {
        "id": "nak7gT5LSCI5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to extract article text for each paragraph with their respective headings\n",
        "def extract_text_with_headings(soup):\n",
        "    content = {}\n",
        "    current_heading = None\n",
        "\n",
        "    # All relevant headings are <h2> and <h3>, and paragraphs are in <p> tags\n",
        "    for tag in soup.find_all(['h2', 'h3', 'p']):\n",
        "        if tag.name in ['h2', 'h3']:\n",
        "            # Update the current heading, stripping references like \"[edit]\"\n",
        "            current_heading = tag.text.strip().replace('[edit]', '')\n",
        "        elif tag.name == 'p':\n",
        "            # Collect text under the current heading\n",
        "            if current_heading not in content:\n",
        "                content[current_heading] = []\n",
        "            content[current_heading].append(tag.text.strip())\n",
        "\n",
        "    # Combine the list of paragraphs under each heading into a single string\n",
        "    for heading in content:\n",
        "        content[heading] = \"\\n\".join(content[heading])\n",
        "\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "eIlNdmIgSVV3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to collect every link redirecting to another wikipedia page\n",
        "def collect_internal_links(soup):\n",
        "    base_url = \"https://en.wikipedia.org\"\n",
        "    links = set()\n",
        "\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Filter for links that start with '/wiki/' (internal Wikipedia links)\n",
        "        if href.startswith('/wiki/') and not href.startswith('/wiki/Special:'):\n",
        "            links.add(base_url + href)\n",
        "\n",
        "    return links"
      ],
      "metadata": {
        "id": "8hpge7U8UAqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrap all functions into a single function\n",
        "def scrape_wikipedia_page(wiki_url):\n",
        "    # Step 1: Get the HTML content\n",
        "    soup = get_html_content(wiki_url)\n",
        "\n",
        "    # Step 2: Extract the article title\n",
        "    title = extract_article_title(soup)\n",
        "\n",
        "    # Step 3: Extract text for each paragraph with their respective headings\n",
        "    article_content = extract_text_with_headings(soup)\n",
        "\n",
        "    # Step 4: Collect every link redirecting to another Wikipedia page\n",
        "    links = collect_internal_links(soup)\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'content': article_content,\n",
        "        'internal_links': links\n",
        "    }"
      ],
      "metadata": {
        "id": "vzGBEfhTU6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the last function on a wikipedia page of your choice.\n",
        "# Example Wikipedia page to test the function\n",
        "wiki_url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
        "\n",
        "# Test the function\n",
        "result = scrape_wikipedia_page(wiki_url)\n",
        "\n",
        "# Display the results\n",
        "print(\"Title of the Article:\", result['title'])\n",
        "print(\"\\nArticle Content (Headings and Paragraphs):\")\n",
        "for heading, text in result['content'].items():\n",
        "    print(f\"\\n{heading}:\")\n",
        "    print(text[:500] + '...')  # Show the first 500 characters for brevity\n",
        "\n",
        "print(\"\\nNumber of Internal Links Found:\", len(result['internal_links']))\n",
        "print(\"First 5 Internal Links:\")\n",
        "for link in list(result['internal_links'])[:5]:\n",
        "    print(link)"
      ],
      "metadata": {
        "id": "BgXdu4-ZVpG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}